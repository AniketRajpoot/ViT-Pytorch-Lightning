{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViT_PytorchLightning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f0ccca01fc304e218100176282d81573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2e605bb58f3c468f8f18cb2a8ea127aa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_12923c79b8b14fe59236a94d52c005e7",
              "IPY_MODEL_eaef6791a6154f5893717da646484d79"
            ]
          }
        },
        "2e605bb58f3c468f8f18cb2a8ea127aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12923c79b8b14fe59236a94d52c005e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_46ce811e2107482d83933e389af6375e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f453b78669994433b80cdc0f3ff774f6"
          }
        },
        "eaef6791a6154f5893717da646484d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c343197344d5444387fe81a4f0cca04b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:17&lt;00:00, 32281840.60it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30dc03bab3314d3ca93cf376cbec467e"
          }
        },
        "46ce811e2107482d83933e389af6375e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f453b78669994433b80cdc0f3ff774f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c343197344d5444387fe81a4f0cca04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30dc03bab3314d3ca93cf376cbec467e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "feilcF-BME_m",
        "outputId": "1f46ec16-4060-4702-d941-311da14fd303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/99/36ac1fd9516ddd0194afe8f399a1e78b1994cdca412b88823d93b7c3bc03/pytorch_lightning-1.0.5-py3-none-any.whl (559kB)\n",
            "\u001b[K     |████████████████████████████████| 563kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: torch<1.8,>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.18.5)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 22.3MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 28.6MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.33.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.35.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (50.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Building wheels for collected packages: PyYAML, future\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=cced04783e19773e8758f536c2e84f53fc260ed547ef82500eb356db8f19fa9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=e9d7973d1ae9541e820e56e6cd25df8a6656be576f641b51ec0a610351736b02\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built PyYAML future\n",
            "Installing collected packages: PyYAML, future, fsspec, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 fsspec-0.8.4 future-0.18.2 pytorch-lightning-1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3vWUVA8BBem",
        "outputId": "a00d1a94-445d-4848-aa4d-add46c17ee8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDUtJjhGMO0x"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from einops import rearrange, repeat\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6J6CF5_W00Y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FOnW3pHMvQY"
      },
      "source": [
        "B_16 = {\n",
        "    'patch_size':(16,16),\n",
        "    'hidden_size':768,\n",
        "    'mlp_dim':3072,\n",
        "    'num_heads':12,\n",
        "    'num_layers':12,\n",
        "    'attention_dropout_rate':0.0,\n",
        "    'dropout_rate': 0.1,\n",
        "} #representation_size?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awqsPgpIN703"
      },
      "source": [
        "B_32 = {\n",
        "    'patch_size':(32,32),\n",
        "    'hidden_size':768,\n",
        "    'mlp_dim':3072,\n",
        "    'num_heads':12,\n",
        "    'num_layers':12,\n",
        "    'attention_dropout_rate':0.0,\n",
        "    'dropout_rate': 0.1,\n",
        "} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOGIyJSmN984"
      },
      "source": [
        "L_16 = {\n",
        "    'patch_size':(16,16),\n",
        "    'hidden_size':1024,\n",
        "    'mlp_dim':4096,\n",
        "    'num_heads':16,\n",
        "    'num_layers':24,\n",
        "    'attention_dropout_rate':0.0,\n",
        "    'dropout_rate':0.1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3p_Cdl3RXlD"
      },
      "source": [
        "L_32 = {\n",
        "    'patch_size':(32,32),\n",
        "    'hidden_size':1024,\n",
        "    'mlp_dim':4096,\n",
        "    'num_heads':16,\n",
        "    'num_layers':24,\n",
        "    'attention_dropout_rate':0.0,\n",
        "    'dropout_rate':0.1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrfp5VN8MZQP"
      },
      "source": [
        "configs = {\n",
        "    'B_16': B_16,\n",
        "    'B_32': B_32,\n",
        "    'L_16': L_16,\n",
        "    'L_32': L_32\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5xzi2P3Y5a0"
      },
      "source": [
        "#pretraining presets?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSMn8IsMS744"
      },
      "source": [
        "fine_tuning_presets = {\n",
        "    'ImageNet': {'lr':[0.003,0.01,0.03,0.06], 'batch_size':512, 'steps':20000},\n",
        "    'CIFAR100': {'lr':[0.001,0.003,0.01,0.03], 'batch_size':512, 'steps':10000},\n",
        "    'CIFAR10': {'lr':[0.001,0.003,0.01,0.03], 'batch_size':512, 'steps':10000},\n",
        "    'Oxford-IIIT Pets': {'lr':[0.001,0.003,0.01,0.03], 'batch_size':512, 'steps':500},\n",
        "    'Oxford Flowers-102': {'lr':[0.001,0.003,0.01,0.03], 'batch_size':512, 'steps':500},\n",
        "    'VTAB': {'lr':[0.01], 'batch_size':512, 'steps':20000},\n",
        "} #cosine lr decay, grad clipping at global norm 1, SGD with momentum 0.9,  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2KlNEbnas3w"
      },
      "source": [
        "architecture = 'B_16'\n",
        "dataset = 'ImageNet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FcIZMz7MWhO"
      },
      "source": [
        "hparams = {}\n",
        "mode = 'fine-tuning'  #REMOVE\n",
        "if mode == 'fine-tuning':\n",
        "  for k in (configs, fine_tuning_presets):\n",
        "    if architecture in k.keys():\n",
        "      hparams.update(k[architecture])\n",
        "    if dataset in k.keys():\n",
        "      hparams.update(k[dataset])\n",
        "    hparams['optimizer'] = 'SGDm0.9'\n",
        "    hparams['scheduler'] = 'cosine_lr_decay'\n",
        "    #print(k,'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhQmDEVVapZF",
        "outputId": "c5a0eb09-c912-4dcc-fcbd-0940480f8f96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(hparams.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['patch_size', 'hidden_size', 'mlp_dim', 'num_heads', 'num_layers', 'attention_dropout_rate', 'dropout_rate', 'optimizer', 'scheduler', 'lr', 'batch_size', 'steps'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQFn0sw9MY3a",
        "outputId": "3c563c40-46ac-4752-958b-94c0282b7269",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(hparams.values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_values([(16, 16), 768, 3072, 12, 12, 0.0, 0.1, 'SGDm0.9', 'cosine_lr_decay', [0.003, 0.01, 0.03, 0.06], 512, 20000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RqLLk1-GUTK"
      },
      "source": [
        "Doubt: How to implement the linear projection, whose weights are actually (16,16,3,768) in Flax, however, Pytorch has different working of Linear layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt99zKuLfbot",
        "outputId": "1f6e0e0d-c106-4059-c1da-ade2a685cf26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample =  torch.rand((196,16,16,3))\n",
        "linear_layer = nn.Linear(768,768)\n",
        "print(linear_layer(sample.view(196,-1)).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([196, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKgkwFFkJUb0"
      },
      "source": [
        "class MultiheadedSelfAttention(nn.Module):\n",
        "    def __init__(self,embed_size,heads):\n",
        "        super(MultiheadedSelfAttention,self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embed size should be divisible by num heads\"\n",
        "\n",
        "        self.query = nn.Linear(self.head_dim, self.head_dim)\n",
        "        self.key = nn.Linear(self.head_dim, self.head_dim)\n",
        "        self.value = nn.Linear(self.head_dim, self.head_dim)\n",
        "        self.out = nn.Linear(heads*self.head_dim,embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        #no. of training examples in the batch\n",
        "        N = query.shape[0]\n",
        "\n",
        "        #corresponding src or target length \n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        #split embedding into self.heads pieces (one of the faster way to do this)\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.value(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.key(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.query(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        #einsum is multidimensional matrix multiplication \n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\",[queries, keys])\n",
        "        # energy represents the output of Q*K\n",
        "        # queries shape: (N, query_len, heads, heads_dim)\n",
        "        # keys shape: (N, keys_len, heads, heads_dim)\n",
        "\n",
        "        #masking if required \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        #calculating attention \n",
        "        attention = torch.softmax(energy/ (self.embed_size ** (1/2)), dim=3)\n",
        "\n",
        "        #multiplying by the values \n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\",[attention, values]).reshape(\n",
        "            N, query_len, self.heads*self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out shape: (N, query_len, heads, head_dim) then flatten last two dimension to get the final output\n",
        "\n",
        "        out = self.out(out)\n",
        "        return out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RhrgF3dJVNW"
      },
      "source": [
        "class TransformerBlock(nn.Module):  ##MODIFY      #CHECKED\n",
        "    def __init__(self, embed_size, heads, mlp_dim, dropout):\n",
        "        super(TransformerBlock,self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.LayerNorm_0 = nn.LayerNorm(embed_size)\n",
        "        self.MultiHeadDotProductAttention_1 = MultiheadedSelfAttention(embed_size,heads)\n",
        "        self.LayerNorm_2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        #defining feed formward here instead of separate block \n",
        "        #forward expansion is an arbritary choice 4 in paper \n",
        "        self.MlpBlock_3= nn.Sequential(OrderedDict([\n",
        "            ('Dense_0',nn.Linear(embed_size,mlp_dim)),\n",
        "            ('gelu',nn.GELU()),\n",
        "            ('dropout_0',nn.Dropout(dropout)),\n",
        "            ('Dense_1',nn.Linear(mlp_dim,embed_size)),\n",
        "            ('dropout_1',nn.Dropout(dropout))\n",
        "        ]))\n",
        "        \n",
        "\n",
        "    def forward(self, value, key, query, mask):  #INCOMPLETE\n",
        "\n",
        "        q = self.LayerNorm_0(query)\n",
        "        k = self.LayerNorm_0(key)\n",
        "        v = self.LayerNorm_0(value)\n",
        "        #mask?\n",
        "        attention = self.MultiHeadDotProductAttention_1(v, k, q, mask)\n",
        "        attention = nn.Dropout(self.dropout)(attention)\n",
        "\n",
        "        #skip connections as in paper to provide raw context\n",
        "        attention = attention + query\n",
        "\n",
        "        forward = self.LayerNorm_2(attention)\n",
        "        #x = self.dropout(self.LayerNorm_0(attention + query))  #MISTAKE?\n",
        "        forward = self.MlpBlock_3(forward)\n",
        "\n",
        "        #skip connections as in paper to provide raw context\n",
        "        #out = self.dropout(self.LayerNorm_2(attention + query))   #MISTAKE?\n",
        "        out = attention + forward\n",
        "\n",
        "        return out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spkmZsRsI5rl"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_size,\n",
        "        num_layers, \n",
        "        heads,  #number of heads in Multiheaded Attention?\n",
        "        mlp_dim,\n",
        "        dropout,\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.encoder_norm = nn.LayerNorm(embed_size)  #chkpt['Transformer']['encoder_norm']\n",
        "\n",
        "        layers_dict = OrderedDict()\n",
        "        for i in range(num_layers):\n",
        "          layers_dict['encoderblock_'+str(i)] = TransformerBlock(\n",
        "                  embed_size,\n",
        "                  heads,\n",
        "                  mlp_dim = mlp_dim,\n",
        "                  dropout=dropout,\n",
        "              )\n",
        "\n",
        "        self.layers = nn.Sequential(layers_dict)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        print(x.shape)\n",
        "        B, N, seq_length = x.shape   #Batch N = no. of tokens, seq_length = embedding size\n",
        "\n",
        "        for layer in range(len(self.layers)):\n",
        "            x = self.layers[0](x,x,x,mask)\n",
        "\n",
        "        #for layer in self.layers:\n",
        "        #    x = layer(x,x,x,mask)  #WTF?\n",
        "        x = self.encoder_norm(x)\n",
        "\n",
        "        out = x\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E89Sr3HWFCX",
        "outputId": "8c720c96-87b3-45c7-cfd0-63b69eb1184c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#TODO: \n",
        "'''\n",
        "add 'assert' for checking whether input shape is correct\n",
        "remove the hardcoded hparams and fit with entries of self.hparams dictionary\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nadd 'assert' for checking whether input shape is correct\\nremove the hardcoded hparams and fit with entries of self.hparams dictionary\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywf5fQTTRE84",
        "outputId": "bc35be10-10a4-476b-9f28-15ce0c590bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "#CHECK\n",
        "'''\n",
        "1) If the torch.view(-1,16,16,3) divides the image into proper slices or not\n",
        "2) If the 'embedding' parameter of checkpoint dict, i.e, the trainable linear projection is loaded properly as a torch.nn.Linear layer\n",
        "UPDATE; 'embedding' is actually Conv2D of size (16,16) and stride (16,16), inp. channels = 3 and output channels = 768\n",
        "(as it was originally for flax.nn.Linear layer)\n",
        "\n",
        "3) If the 'key','value','query' and 'out' are loaded properly\n",
        "4) How the final MLP block has actually two blocks with GELU activation\n",
        "5) Where 'pre_logits' from the checkpoint dictionary fit into our model\n",
        "6) If 'mask' fits in\n",
        "7) Representation size??\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n1) If the torch.view(-1,16,16,3) divides the image into proper slices or not\\n2) If the 'embedding' parameter of checkpoint dict, i.e, the trainable linear projection is loaded properly as a torch.nn.Linear layer\\nUPDATE; 'embedding' is actually Conv2D of size (16,16) and stride (16,16), inp. channels = 3 and output channels = 768\\n(as it was originally for flax.nn.Linear layer)\\n\\n3) If the 'key','value','query' and 'out' are loaded properly\\n4) How the final MLP block has actually two blocks with GELU activation\\n5) Where 'pre_logits' from the checkpoint dictionary fit into our model\\n6) If 'mask' fits in\\n7) Representation size??\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I80fkovYbKcM"
      },
      "source": [
        "class ViT(pl.LightningModule):\n",
        "  def __init__(self,hparams, pretrained_path = None):\n",
        "    super(ViT, self).__init__()\n",
        "    self.patch_size = 16 #change to self.hparams.patch_size[0]\n",
        "\n",
        "    #SAMPLE JUST ASSUMING THAT A SINGLE IMAGE IS PASSES, WHEREAS A BATCH WOULD BE PASSED IN REAL `forward` METHOD\n",
        "    img = torch.rand((3,224,224))\n",
        "    img = img.permute(1,2,0)\n",
        "    print(\"img shape:\",img.shape)\n",
        "    img = img.reshape(-1,3,16,16)\n",
        "    print(\"number of tokens:\",img.shape[0])\n",
        "    \n",
        "    #trainable linear projection\n",
        "    #self.embedding = nn.Linear(768,768,True) #output= 32x196x768 if batch size is 32?\n",
        "    self.embedding = nn.Conv2d(3,768,(16,16),(16,16), padding_mode = 'zeros')\n",
        "    self.patch_to_embedding = nn.Linear(768, 768)\n",
        "\n",
        "    #self.lin_projection.data = self.loaded_weights['embedding']['kernel'].reshape(-1,768) where 768 is self.hparams['hidden_size']\n",
        "    self.cls = nn.Parameter(data = torch.rand((1,1,768)))\n",
        "    \n",
        "    self.Transformer = Encoder(embed_size = 768, num_layers= 12, heads= 12, mlp_dim= 3072, dropout = 0.0)\n",
        "    \n",
        "    lin_proj = self.embedding(img).unsqueeze(0)\n",
        "    print('lin proj shape:',lin_proj.shape)\n",
        "    lin_proj = lin_proj.squeeze(3).squeeze(3)\n",
        "\n",
        "    final_tokens = torch.cat((self.cls.data, lin_proj), dim=1)\n",
        "    print(\"Final token size:\", final_tokens.shape)\n",
        "\n",
        "    self.pos_embedding = nn.Parameter(data = torch.rand((1,197,768)))\n",
        "    self.use_conv = True\n",
        "    self.pre_logits = nn.Linear(768,768)\n",
        "    self.head = nn.Linear(768, 21843)\n",
        "\n",
        "    result = self.head(self.pre_logits(self.Transformer(final_tokens+ self.pos_embedding)[:,0,:]))\n",
        "    print(result.shape)\n",
        "\n",
        "  def forward(self, img, mask = None):  \n",
        "    \n",
        "    p = self.patch_size\n",
        "    #img = img.reshape(img.shape[0],-1,3,p,p)\n",
        "\n",
        "    #using convolution or simple linear layer for projection to right dimensions \n",
        "    if (self.use_conv):                \n",
        "        #img = rearrange(img, 'b c h w -> b (h w) c')\n",
        "        x = self.embedding(img)\n",
        "        print(x.shape)\n",
        "        x = x.view(x.shape[0], -1, x.shape[1])\n",
        "        print(x.shape)               \n",
        "    else :\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "    #getting batch and sequence length essentially\n",
        "    b, n, _ = x.shape\n",
        "    \n",
        "\n",
        "    #this part appends the class tokens and adds positional embedding \n",
        "    #cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "    #x = torch.cat((cls_tokens, x), dim=1)\n",
        "    batch_cls = torch.repeat_interleave(self.cls.data, 32, 0)\n",
        "    final_tokens = torch.cat((batch_cls, x), dim=1)\n",
        "\n",
        "    #positions = torch.arange(0, n + 1).expand(N, n + 1).to(self.device)\n",
        "    #x = self.dropout(x + self.pos_embedding(positions))\n",
        "    #as implemented in a default way!\n",
        "    \n",
        "    #x += self.pos_embedding[:, :(n + 1)]\n",
        "    batch_pos_embedding = torch.repeat_interleave(self.pos_embedding, 32, 0)\n",
        "    print(batch_pos_embedding.shape)\n",
        "\n",
        "    final_tokens = final_tokens + batch_pos_embedding\n",
        "\n",
        "    #calling the transformer here!\n",
        "    classes = self.head(self.pre_logits(self.Transformer(final_tokens)[:,0,:]))\n",
        "\n",
        "    \n",
        "    #x = self.transformer(x, mask)\n",
        "    #x = self.to_cls_token(x[:, 0])\n",
        "    #return self.classifier(x)\n",
        "\n",
        "    return classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNBQrJphWLZP",
        "outputId": "538394be-ca0c-4e4b-b873-1143a508e1ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = ViT(hparams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img shape: torch.Size([224, 224, 3])\n",
            "number of tokens: 196\n",
            "lin proj shape: torch.Size([1, 196, 768, 1, 1])\n",
            "Final token size: torch.Size([1, 197, 768])\n",
            "torch.Size([1, 197, 768])\n",
            "torch.Size([1, 21843])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIyR7FGT_cqs",
        "outputId": "0a636ca8-1013-448d-9c85-78b1306ac9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img = torch.rand((32,3,224,224))\n",
        "print(model(img).shape)\n",
        "#SUCCESS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 768, 14, 14])\n",
            "torch.Size([32, 196, 768])\n",
            "torch.Size([32, 197, 768])\n",
            "torch.Size([32, 197, 768])\n",
            "torch.Size([32, 21843])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20hZsOK0FhNU"
      },
      "source": [
        "chkpt = load('/content/drive/My Drive/imagenet21k_ViT-B_16.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im2ej_U3WBJi"
      },
      "source": [
        "Sandbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejGZcgNbGegR"
      },
      "source": [
        "print(model.Transformer.layers[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIUKw6xqLyUa",
        "outputId": "a94780f7-efbb-4ea8-b9d1-7130cc4158c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "my_str = 'ayy.beee.siii'\n",
        "print(len(my_str.split('.')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdID1TixQeZ1",
        "outputId": "8b7ba809-07a1-4ab7-bd73-c6d124a9dc4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.Transformer.layers.encoderblock_0.LayerNorm_0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LayerNorm((768,), eps=1e-05, elementwise_affine=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T78xQZ7MRwCq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYYCiEUGPf8E"
      },
      "source": [
        "my_obj = nn.LayerNorm(768)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lreM2dGHMHEA"
      },
      "source": [
        "print(getattr(model,'embedding').weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcpBD4SPV7LS"
      },
      "source": [
        "Loading model checkpoints into out Lightning Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8DDpqQAFSwZ",
        "outputId": "3cb90754-1f4d-4800-b082-e45f18fdd58f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "  name = name.split('.')\n",
        "  if len(name) == 1:   #for 'cls' and 'pos_embedding'\n",
        "    if name[0] == 'pos_embedding':\n",
        "      getattr(model, name[0]).data = torch.Tensor(chkpt['Transformer']['posembed_input'][name[0]])\n",
        "    elif name[0] == 'cls':\n",
        "      getattr(model, name[0]).data = torch.Tensor(chkpt[name[0]])              \n",
        "  elif len(name) == 2 and 'patch_to_embedding' not in name:  #for 'pre_logits', 'head', 'embedding', as 'patch_to_embedding' is additionally defined by us\n",
        "    getattr(model,name[0]).weight.data = torch.Tensor(chkpt[name[0]]['kernel'])\n",
        "    getattr(model,name[0]).bias.data = torch.Tensor(chkpt[name[0]]['bias'])\n",
        "  elif len(name) == 3: #for 'Transformer.encoder_norm'\n",
        "    getattr(getattr(model,name[0]),name[1]).weight.data = torch.Tensor(chkpt[name[0]][name[1]]['scale'])\n",
        "    getattr(getattr(model,name[0]),name[1]).bias.data = torch.Tensor(chkpt[name[0]][name[1]]['bias'])\n",
        "  elif len(name) == 5: #for Transformer.layers.encoderblock_i.LayerNorm0 or 2, i.e, name[3] guaranteed to be 'LayerNorm_0' or 'LayerNorm_2'\n",
        "    getattr(getattr(getattr(getattr(model,name[0]),name[1]),name[2]),name[3]).weight.data = torch.Tensor(chkpt[name[0]][name[2]][name[3]]['scale'])\n",
        "    getattr(getattr(getattr(getattr(model,name[0]),name[1]),name[2]),name[3]).bias.data = torch.Tensor(chkpt[name[0]][name[2]][name[3]]['bias'])\n",
        "  elif len(name) == 6:\n",
        "    getattr(getattr(getattr(getattr(getattr(model,name[0]),name[1]),name[2]),name[3]),name[4]).weight.data = torch.Tensor(chkpt[name[0]][name[2]][name[3]][name[4]]['kernel'])\n",
        "    getattr(getattr(getattr(getattr(getattr(model,name[0]),name[1]),name[2]),name[3]),name[4]).bias.data = torch.Tensor(chkpt[name[0]][name[2]][name[3]][name[4]]['bias'])\n",
        "  else:\n",
        "    print(len(name))\n",
        "    print(name)\n",
        "    print('Understandable, have a nice day')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "['patch_to_embedding', 'weight']\n",
            "Understandable, have a nice day\n",
            "2\n",
            "['patch_to_embedding', 'bias']\n",
            "Understandable, have a nice day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwpI19QwWred"
      },
      "source": [
        "Testing whether the loaded weights are correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS84g8WAXFMd",
        "outputId": "6c772c4d-7fd4-4f97-d8a4-8e12fd172899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/vision_transformer'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKvIWj3fXHwg",
        "outputId": "cddbc3bf-0bfe-4ea9-f197-6891e194988a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd '/content'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VP2TCJxXKPK",
        "outputId": "09914500-b5ab-4ffd-f3ff-50a8ec459d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCkxElkocfFa",
        "outputId": "73742e65-78a6-4fd7-d214-ce1e79aedb43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpqGXmDwbIKO",
        "outputId": "876b7649-df01-4f20-c311-81d3c835c1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#https://storage.googleapis.com/bit_models/imagenet21k_wordnet_ids.txt\n",
        "from nltk.corpus import wordnet as wn\n",
        "wordnet_ids = open(\"/content/imagenet21k_wordnet_ids.txt\", \"r\").readlines()\n",
        "wordnet_ids = [x.strip() for x in wordnet_ids]\n",
        "wordnet_ids = [int(x[1:]) for x in wordnet_ids]\n",
        "\n",
        "print(wn.synset_from_pos_and_offset('n', wordnet_ids[21800]))\n",
        "\n",
        "'''\n",
        "for index in torch.argsort(output, descending=True)[:10]:\n",
        "    print(wn.synset_from_pos_and_offset('n', wordnet_ids[int(index)]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('pantothenic_acid.n.01')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor index in torch.argsort(output, descending=True)[:10]:\\n    print(wn.synset_from_pos_and_offset('n', wordnet_ids[int(index)]))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHdsd5TKeUB0"
      },
      "source": [
        "make_tensor = transforms.Compose([\n",
        "  transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF-h-p1kWx1L",
        "outputId": "d7ac6df9-32e4-40e2-8c6b-e8b818a7cb59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "f0ccca01fc304e218100176282d81573",
            "2e605bb58f3c468f8f18cb2a8ea127aa",
            "12923c79b8b14fe59236a94d52c005e7",
            "eaef6791a6154f5893717da646484d79",
            "46ce811e2107482d83933e389af6375e",
            "f453b78669994433b80cdc0f3ff774f6",
            "c343197344d5444387fe81a4f0cca04b",
            "30dc03bab3314d3ca93cf376cbec467e"
          ]
        }
      },
      "source": [
        "dataset = datasets.CIFAR10(root='.', transform= make_tensor, download = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0ccca01fc304e218100176282d81573",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VurIF4GXoiP"
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset,batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmN2TnwWxkk",
        "outputId": "bf2a4e5f-5be2-44d9-ae37-a6b72f3e7dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class_to_idx = dataset.class_to_idx\n",
        "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "print(type(idx_to_class))\n",
        "print(idx_to_class.keys())\n",
        "print(idx_to_class.values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "dict_values(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SZ8-h7tWxV1",
        "outputId": "972b8caf-3dd0-405e-82d5-c337bcd1cde7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "i = 0\n",
        "for batch, labels in dataloader:\n",
        "  i = i + 1\n",
        "  print(batch.shape)\n",
        "  print(labels.shape)\n",
        "  plt.imshow(batch[0].permute)\n",
        "  if i == 6:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-206-04984d1ae151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    692\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m    693\u001b[0m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0;32m--> 694\u001b[0;31m                             \"float\".format(self._A.dtype))\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         if not (self._A.ndim == 2\n",
            "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiF3AIGEWwzR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTIf3hYKEkiQ"
      },
      "source": [
        "for name in chkpt['Transformer'].keys():\n",
        "  print(name,'\\'s:',chkpt['Transformer'][name].keys())\n",
        "  for x in chkpt['Transformer'][name].keys():\n",
        "    if (type(chkpt['Transformer'][name][x]) is not dict):\n",
        "      print('Dimension of', x, ':', chkpt['Transformer'][name][x].shape)\n",
        "    else:\n",
        "      for y in chkpt['Transformer'][name][x].keys():\n",
        "        if (type(chkpt['Transformer'][name][x][y])) is not dict:\n",
        "          print('Dimension of',x,'\\'s',y,'is ', chkpt['Transformer'][name][x][y].shape)\n",
        "        else:\n",
        "          print('\\n',x,'\\'s',y,'dictionary is ', chkpt['Transformer'][name][x][y].keys())\n",
        "          print('------->')\n",
        "          for z in chkpt['Transformer'][name][x][y].keys():\n",
        "            print('\\t',z,'dictionary\\'s entries\\' dims are:',chkpt['Transformer'][name][x][y][z].shape)\n",
        "  print('-----------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRumOk_-WU1z",
        "outputId": "aa312a9d-4417-4098-f439-dc7d69f82377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cls\n",
            "pos_embedding\n",
            "embedding.weight\n",
            "embedding.bias\n",
            "patch_to_embedding.weight\n",
            "patch_to_embedding.bias\n",
            "Transformer.encoder_norm.weight\n",
            "Transformer.encoder_norm.bias\n",
            "Transformer.layers.encoderblock_0.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_0.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_0.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_0.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_0.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_0.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_0.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_0.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_0.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_0.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_0.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_0.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_0.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_0.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_0.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_0.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_1.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_1.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_1.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_1.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_1.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_1.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_1.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_1.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_1.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_1.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_1.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_1.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_1.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_1.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_1.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_1.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_2.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_2.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_2.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_2.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_2.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_2.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_2.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_2.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_2.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_2.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_2.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_2.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_2.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_2.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_2.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_2.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_3.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_3.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_3.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_3.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_3.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_3.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_3.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_3.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_3.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_3.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_3.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_3.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_3.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_3.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_3.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_3.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_4.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_4.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_4.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_4.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_4.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_4.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_4.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_4.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_4.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_4.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_4.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_4.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_4.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_4.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_4.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_4.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_5.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_5.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_5.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_5.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_5.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_5.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_5.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_5.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_5.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_5.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_5.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_5.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_5.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_5.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_5.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_5.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_6.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_6.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_6.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_6.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_6.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_6.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_6.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_6.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_6.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_6.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_6.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_6.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_6.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_6.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_6.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_6.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_7.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_7.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_7.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_7.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_7.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_7.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_7.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_7.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_7.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_7.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_7.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_7.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_7.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_7.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_7.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_7.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_8.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_8.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_8.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_8.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_8.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_8.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_8.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_8.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_8.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_8.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_8.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_8.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_8.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_8.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_8.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_8.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_9.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_9.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_9.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_9.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_9.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_9.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_9.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_9.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_9.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_9.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_9.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_9.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_9.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_9.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_9.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_9.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_10.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_10.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_10.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_10.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_10.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_10.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_10.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_10.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_10.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_10.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_10.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_10.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_10.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_10.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_10.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_10.MlpBlock_3.Dense_1.bias\n",
            "Transformer.layers.encoderblock_11.LayerNorm_0.weight\n",
            "Transformer.layers.encoderblock_11.LayerNorm_0.bias\n",
            "Transformer.layers.encoderblock_11.MultiHeadDotProductAttention_1.query.weight\n",
            "Transformer.layers.encoderblock_11.MultiHeadDotProductAttention_1.query.bias\n",
            "Transformer.layers.encoderblock_11.MultiHeadDotProductAttention_1.key.weight\n",
            "Transformer.layers.encoderblock_11.MultiHeadDotProductAttention_1.key.bias\n",
            "Transformer.layers.encoderblock_11.MultiHeadDotProductAttention_1.value.weight\n",
            "Transformer.layers.encoderblock_11.MultiHeadDotProductAttention_1.value.bias\n",
            "Transformer.layers.encoderblock_11.MultiHeadDotProductAttention_1.out.weight\n",
            "Transformer.layers.encoderblock_11.MultiHeadDotProductAttention_1.out.bias\n",
            "Transformer.layers.encoderblock_11.LayerNorm_2.weight\n",
            "Transformer.layers.encoderblock_11.LayerNorm_2.bias\n",
            "Transformer.layers.encoderblock_11.MlpBlock_3.Dense_0.weight\n",
            "Transformer.layers.encoderblock_11.MlpBlock_3.Dense_0.bias\n",
            "Transformer.layers.encoderblock_11.MlpBlock_3.Dense_1.weight\n",
            "Transformer.layers.encoderblock_11.MlpBlock_3.Dense_1.bias\n",
            "pre_logits.weight\n",
            "pre_logits.bias\n",
            "head.weight\n",
            "head.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnt2Wf_SY1MG"
      },
      "source": [
        "!pip install flax\n",
        "!pip install ml_collections\n",
        "!git clone https://github.com/google-research/vision_transformer.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6s-TJPZZDMU",
        "outputId": "22e78997-a671-4ced-edc5-db50d34eef9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd vision_transformer/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/vision_transformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85kVu8rY6xy"
      },
      "source": [
        "from vit_jax import checkpoint_test, checkpoint\n",
        "from vit_jax.checkpoint_test import *\n",
        "from vit_jax.checkpoint import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARfiS9jmY9_5"
      },
      "source": [
        "chkpt = load('/content/drive/My Drive/imagenet21k_ViT-B_16.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-LnXdyBZHqR",
        "outputId": "308a7382-50af-4867-ac93-393409c39431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chkpt.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['cls', 'Transformer', 'embedding', 'head', 'pre_logits'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PMCnjtJZR8q",
        "outputId": "410b1aff-65c0-4e39-e7fc-6f6de4f988e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chkpt['Transformer'].keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['encoder_norm', 'encoderblock_0', 'encoderblock_1', 'encoderblock_10', 'encoderblock_11', 'encoderblock_2', 'encoderblock_3', 'encoderblock_4', 'encoderblock_5', 'encoderblock_6', 'encoderblock_7', 'encoderblock_8', 'encoderblock_9', 'posembed_input'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Po36npZTvy",
        "outputId": "8b03c1c2-4db5-4b3e-d536-61dc9dfb8003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for name in chkpt['Transformer'].keys():\n",
        "  print('----------------')\n",
        "  print(name,'\\'s:',chkpt['Transformer'][name].keys())\n",
        "  print('>>')\n",
        "  for x in chkpt['Transformer'][name].keys():\n",
        "    if (type(chkpt['Transformer'][name][x]) is not dict):\n",
        "      print('Dimension of', x, ':', chkpt['Transformer'][name][x].shape)\n",
        "    else:\n",
        "      for y in chkpt['Transformer'][name][x].keys():\n",
        "        if (type(chkpt['Transformer'][name][x][y])) is not dict:\n",
        "          print('Dimension of',x,'\\'s',y,'is ', chkpt['Transformer'][name][x][y].shape)\n",
        "        else:\n",
        "          print('\\n',x,'\\'s',y,'dictionary is ', chkpt['Transformer'][name][x][y].keys())\n",
        "          print('------->')\n",
        "          for z in chkpt['Transformer'][name][x][y].keys():\n",
        "            print('\\t',z,'dictionary\\'s entries\\' dims are:',chkpt['Transformer'][name][x][y][z].shape)\n",
        "  print('-----------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------\n",
            "encoder_norm 's: dict_keys(['bias', 'scale'])\n",
            ">>\n",
            "Dimension of bias : (768,)\n",
            "Dimension of scale : (768,)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_0 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_1 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_10 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_11 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_2 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_3 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_4 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_5 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_6 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_7 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_8 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "encoderblock_9 's: dict_keys(['LayerNorm_0', 'LayerNorm_2', 'MlpBlock_3', 'MultiHeadDotProductAttention_1'])\n",
            ">>\n",
            "Dimension of LayerNorm_0 's bias is  (768,)\n",
            "Dimension of LayerNorm_0 's scale is  (768,)\n",
            "Dimension of LayerNorm_2 's bias is  (768,)\n",
            "Dimension of LayerNorm_2 's scale is  (768,)\n",
            "\n",
            " MlpBlock_3 's Dense_0 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (3072,)\n",
            "\t kernel dictionary's entries' dims are: (768, 3072)\n",
            "\n",
            " MlpBlock_3 's Dense_1 dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (3072, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's key dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's out dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (768,)\n",
            "\t kernel dictionary's entries' dims are: (12, 64, 768)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's query dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "\n",
            " MultiHeadDotProductAttention_1 's value dictionary is  dict_keys(['bias', 'kernel'])\n",
            "------->\n",
            "\t bias dictionary's entries' dims are: (12, 64)\n",
            "\t kernel dictionary's entries' dims are: (768, 12, 64)\n",
            "-----------------\n",
            "----------------\n",
            "posembed_input 's: dict_keys(['pos_embedding'])\n",
            ">>\n",
            "Dimension of pos_embedding : (1, 197, 768)\n",
            "-----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrVvctzEZU5K",
        "outputId": "ea0b9780-42fc-478a-a6b4-5533b6d55023",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for x in chkpt['embedding'].values():\n",
        "  print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768,)\n",
            "(16, 16, 3, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN3Mi-qZBvkM",
        "outputId": "49890d2c-0b6d-4090-bc1a-8b4a72046c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chkpt['pre_logits'].keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['bias', 'kernel'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    }
  ]
}